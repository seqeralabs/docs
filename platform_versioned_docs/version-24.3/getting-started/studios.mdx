---
title: "Studios for interactive analysis"
description: "An tutorial for creating interactive analysis Studios for Jupyter, RStudio, VSCode, and more"
date: "17 Feb 2025"
tags: [platform, studios, data studios, jupyter, rstudio, xpra, vscode, conda]
toc_max_heading_level: 2
---

Seqera Studios allows users to host a variety of container images in Seqera Platform compute environments for dynamic analysis using popular tools such as Jupyter and RStudio notebooks, Visual Studio Code IDEs, and Xpra remote desktops. Each studio session provides an individual interactive environment that encapsulates the live environment for real-time data analysis.

This guide explores how Studios seamlessly integrates with your existing workflows, bridging the gap between pipeline execution and interactive analysis. It details how to set up and use different types of Studios, showcasing real-world use cases that highlight the power and flexibility that Studios can bring to your analysis workflows. 

:::info[**Prerequisites**]
You will need the following to get started:

- Valid credentials for your cloud storage account and compute environment
- To create and configure Studios, you need at least the "Maintain" workspace [user role](../orgs-and-teams/roles.mdx)
- An AWS Batch workspace compute environment (**without Fargate**) with sufficient resources (minimum: 2 CPUs, 8192 MB memory)
- [Data Explorer](../data/data-explorer.mdx) enabled in your workspace
:::

### Jupyter: Python-based visualization of protein structure prediction data 

Studios and Jupyter notebooks enable interactive analysis using Python libraries and tools. For example, PyMOL is a powerful tool used for visualizing and comparing structures produced by workflows such as [nf-core/proteinfold](https://nf-co.re/proteinfold/1.1.1), a bioinformatics best-practice analysis pipeline for protein 3D structure prediction. This section demonstrates how to create an AWS Batch compute environment, add the nf-core AWS megatests public proteinfold data to your workspace, create a Jupyter studio, and run the provided Python script to produce interactive composite 3D images of the H1065 sequence. 

:::note
This script and instructions can also be used to visualize the structures from the results of nf-core/proteinfold runs with your own public or private data. 
:::

#### Create an AWS Batch compute environment 

Studios requires an AWS Batch compute environment. If you do not have an existing compute environment available, create one with the following attributes:

- **Region**: To reduce costs, your compute environment should be in the same region as your data. To browse the nf-core AWS megatests public data optimally, select **eu-west-1**.
- **Provisioning model**: Use **On-demand** EC2 instances. 
- Studios does not support AWS Fargate. Do not enable **Use Fargate for head job**. 
- At least 2 available CPUs and 8192 MB of RAM. 

#### Add public data using Data Explorer

For the purposes of this guide, add the proteinfold results (H1065 sequence) from the nf-core AWS megatests S3 bucket to your workspace using Data Explorer:

1. From the **Data Explorer** tab, select **Add cloud bucket**. 
1. Specify the bucket details:
    - **Provider**: AWS
    - **Bucket path**: `s3://nf-core-awsmegatests/proteinfold/results-9bea0dc4ebb26358142afbcab3d7efd962d3a820/`
    - A unique **Name** for the bucket, such as `nf-core-awsmegatests-proteinfold`.
    - **Credentials**: Select **Public** from the dropdown menu.
    - An optional bucket **Description**.
1. Select **Add**.

:::info 
To use your own pipeline data for interactive visualization, add the cloud bucket that contains the results of your nf-core/proteinfold pipeline run. See [Add a cloud bucket](./quickstart-demo/add-data.mdx#add-a-cloud-bucket) for more information. 
:::

#### Create a Jupyter notebook studio

From the **Data Studios** tab, select **Add a data studio** and complete the following:
- In the **Compute & Data** tab:
    - Select an AWS Batch compute environment. 
        :::info
        The same compute environment can be used for pipeline execution and running your Studios notebook environment, but Studios does not support AWS Fargate. To use one compute environment for both nf-core/proteinfold pipeline execution and your studio, leave **Enable Fargate for head job** disabled. Alternatively, create a second basic AWS Batch compute environment with at least 2 CPUs and 8192 MB of RAM for your studio.
        :::
    - Optional: Enter CPU and memory allocations. The default values are 2 CPUs and 8192 MB memory (RAM).
        :::note
        Studios compete for computing resources when sharing compute environments. Ensure your shared compute environment has sufficient resources to run both your pipelines and studio sessions. 
        :::
    - Mount data using Data Explorer: Mount the S3 bucket or directory path that contains the nf-core AWS megatests proteinfold data, or the pipeline work directory of your nf-core/proteinfold run. 
- In the **General config** tab:
    - Select the latest **Jupyter** container image template from the list.
    - Optional: Enter a unique name and description for the data studio. 
    - Check **Install Conda packages** and paste the following Conda environment YAML snippet:

    ```yaml 
    channels:
    - schrodinger
    - conda-forge
    - bioconda
    dependencies:
    - python=3.10
    - schrodinger::pymol-bundle
    - conda-forge::libgl
    - pip
    - pip:
      - biopython==1.85
      - mdtraj==1.10.3
    ```

- Confirm the data studio details in the **Summary** tab.
- Select **Add** or choose to **Add and start** the studio immediately.
- If you chose to **Add** the studio in the preceding step, select **Connect** in the options menu to open the studio in a new browser tab. 

![Add data studio](./_images/add-ds-pf.gif)

#### Visualize protein structures 

The following Python script visualizes and compares protein structures produced by Alphafold 2 and ESMFold, creating a composite interactive 3D image of the two structures with contrasting colors. The script retrieves the CA atoms from both structures, checks whether the structures have the same number of CA atoms, creates a contact map, calculates the per-residue RMSD for each structure, and visualizes the structures, secondary structures, and per-residue RMSD using PyMOL. 

Run the following scripts in individual code cells in your Jupyter notebook to install the necessary packages and perform visualization:


<details>
<summary>Python script</summary>

    1. Import libraries:

        ```python 
        import py3Dmol
        from IPython.display import display
        from Bio import PDB
        from Bio.PDB import Superimposer
        import numpy as np
        ```

    1. Set up PDB file paths and initialize the PDB parser: 

        ```python 
        alphafold2_multimer_standard = "/workspace/data/proteinfold_1_1_1_results/mode_alphafold2_multimer/alphafold2/standard/H1065.alphafold.pdb"
        esmfold_multimer = "/workspace/data/proteinfold_1_1_1_results/mode_esmfold_multimer/esmfold/H1065.pdb"

        parser = PDB.PDBParser(QUIET=True)
        ```

    1. Load the two structures from your PDB files: 

        ```python 
        structure1 = parser.get_structure("protein1", alphafold2_multimer_standard)
        structure2 = parser.get_structure("protein2", esmfold_multimer)
        ```

    1. Get CA atoms from both structures: 

        ```python 
        ca_atoms_1 = [atom for atom in structure1.get_atoms() if atom.name == 'CA']
        ca_atoms_2 = [atom for atom in structure2.get_atoms() if atom.name == 'CA']
        ```

    1. Check if the structures have the same number of CA atoms, else superimpose the atoms and calculate the RMSD: 

        ```python 
        if len(ca_atoms_1) != len(ca_atoms_2):
            print(f"Warning: Structures have different numbers of CA atoms")
            print(f"Structure 1: {len(ca_atoms_1)} CA atoms")
            print(f"Structure 2: {len(ca_atoms_2)} CA atoms")
        else:
            # Initialize Superimposer
            super_imposer = PDB.Superimposer()
            
            # Set the atoms to be superimposed
            super_imposer.set_atoms(ca_atoms_1, ca_atoms_2)
            
            # Calculate RMSD
            rmsd = super_imposer.rms
            
            print(f"RMSD between structures: {rmsd:.2f} Ã…")
        ```

    1. Compare secondary structures: 

        ```python 
        def compare_secondary_structure(pdb1, pdb2):
            dssp1 = dssp_dict_from_pdb_file(pdb1)[0]
            dssp2 = dssp_dict_from_pdb_file(pdb2)[0]
            
            matches = 0
            total = min(len(dssp1), len(dssp2))
            
            for res1, res2 in zip(dssp1.keys(), dssp2.keys()):
                if dssp1[res1] == dssp2[res2]:
                    matches += 1
            
            return matches / total * 100
        ```

    1. Calculate contact maps: 

        ```python 
        def calculate_contact_map(structure, threshold=8.0):
            ca_atoms = [atom for atom in structure.get_atoms() if atom.name == 'CA']
            n = len(ca_atoms)
            contact_map = np.zeros((n, n))
            
            for i, atom1 in enumerate(ca_atoms):
                for j, atom2 in enumerate(ca_atoms):
                    dist = atom1 - atom2
                    contact_map[i,j] = 1 if dist < threshold else 0
                    
            return contact_map
        ```

    1. Calculate contact map similarity: 

        ```python 
        contact_map1 = calculate_contact_map(structure1)
        contact_map2 = calculate_contact_map(structure2)
        contact_similarity = np.sum(contact_map1 == contact_map2) / contact_map1.size
        ```

    1. Calculate the per-residue RMSD: 

        ```python 
        def calculate_per_residue_rmsd(struct1, struct2):
            per_res_rmsd = []
            for (res1, res2) in zip(struct1.get_residues(), struct2.get_residues()):
                atoms1 = [atom.get_coord() for atom in res1 if atom.name in ['N', 'CA', 'C', 'O']]
                atoms2 = [atom.get_coord() for atom in res2 if atom.name in ['N', 'CA', 'C', 'O']]
                if len(atoms1) == len(atoms2):
                    rmsd = np.sqrt(np.mean(np.sum((np.array(atoms1) - np.array(atoms2))**2, axis=1)))
                    per_res_rmsd.append(rmsd)
            return per_res_rmsd
        ```

    1. Calculate and print additional metrics: 

        ```python 
        ss_agreement = compare_secondary_structure("protein1.pdb", "protein2.pdb")
        per_res_rmsd = calculate_per_residue_rmsd(structure1, structure2)

        print(f"Secondary structure agreement: {ss_agreement:.1f}%")
        print(f"Contact map similarity: {contact_similarity:.3f}")
        print(f"Average per-residue RMSD: {np.mean(per_res_rmsd):.2f} Ã…")
        ```

    1. Visualize structures in PyMOL:

        ```python 
        cmd.load("protein1.pdb", "protein1")
        cmd.load("protein2.pdb", "protein2")
        ```

    1. Apply contrasting structure colors:

        ```python 
        cmd.color("cyan", "protein1")
        cmd.color("magenta", "protein2")
        ```

    1. Align structures: 

        ```python 
        cmd.align("protein1", "protein2")
        ```

    1. Set visualization parameters:

        ```python 
        cmd.show("cartoon")
        cmd.zoom("all")
        ```

    1. (Optional) Save the session:

        ```python 
        cmd.save("comparison.pse")
        ```

    1. Visualize the per-residue RMSD in PyMOL:

        ```python 
        cmd.alter("protein1", f"b={per_res_rmsd}")
        cmd.spectrum("b", "blue_white_red", "protein1")
        cmd.show("cartoon")
        ```
</details>    

### RStudio: Analyze RNASeq data and differential expression statistics 

In this section, we have run the **nf-core/rnaseq** pipeline to quantify gene expression in RNA sequencing data, followed by **nf-core/differentialabundance** to derive differential expression statistics. This section demonstrates how to create a studio to perform further analysis with these results from cloud storage. One of these outputs is an RShiny application that can be deployed for interactive analysis.

#### Create an AWS Batch compute environment 

Studios requires an AWS Batch compute environment. If you do not have an existing compute environment with at least 2 CPUs available, create one with the following attributes:

- **Region**: To reduce costs, your compute environment should be in the same region as your data. To browse the nf-core AWS megatests public data optimally, select **eu-west-1**.
- **Provisioning model**: Use **On-demand** EC2 instances. 
- Studios does not support AWS Fargate. Do not enable **Use Fargate for head job**. 
- At least 2 available CPUs.

#### Add public data using Data Explorer

For the purposes of this guide, add the nf-core AWS megatests S3 bucket to your workspace using Data Explorer:

1. From the **Data Explorer** tab, select **Add cloud bucket**. 
1. Specify the bucket details:
    - **Provider**: AWS
    - **Bucket path**: `s3://nf-core-awsmegatests`
    - A unique **Name** for the bucket, such as `nf-core-awsmegatests`.
    - **Credentials**: Select **Public** from the dropdown menu.
    - An optional bucket **Description**.
1. Select **Add**.

:::info 
To use your own pipeline data for interactive analysis, add the cloud bucket that contains the results of your nf-core/differentialabundance pipeline run. See [Add a cloud bucket](./quickstart-demo/add-data.mdx#add-a-cloud-bucket) for more information. 
:::

#### Create an RStudio notebook studio 

From the **Data Studios** tab, select **Add a data studio** and complete the following:
- Select the latest **RStudio** container image template from the list.
- Select your AWS Batch compute environment. 
:::note
Data studios compete for computing resources when sharing compute environments. Ensure your compute environment has sufficient resources to run both your pipelines and data studio sessions. The default CPU and memory allocation for a data studio is 2 CPUs and 8192 MB RAM. 
:::
- Mount data using Data Explorer: Mount the nf-core AWS megatests S3 bucket, or the directory path that contains the results of your nf-core/differentialabundance pipeline run. 
- Optional: Enter CPU and memory allocations. The default values are 2 CPUs and 8192 MB memory (RAM).
- Select **Add** or choose to **Add and start** the studio immediately.
- If you chose to **Add** the studio in the preceding step, select **Start** in the options menu, then **Connect** to open the studio in a new browser tab when it is running. 

![Add data studio](./_images/create-ds.gif)

#### Perform the analysis and explore results

1. Configure the RStudio environment with installed packages, including ShinyNGS:

    ```r 
    if (!require("BiocManager", quietly = TRUE))
      install.packages("BiocManager")

    BiocManager::install(version = "3.11", ask = FALSE)
    BiocManager::install(c("SummarizedExperiment", "GSEABase", "limma"))

    install.packages(c("devtools", "matrixStats", "rmarkdown", "markdown"))
    install.packages("shiny", repos = "https://cran.rstudio.com/")

    devtools::install_version("cpp11", version = "0.2.1", repos = "http://cran.us.r-project.org")
    devtools::install_github('pinin4fjords/shinyngs', upgrade_dependencies = FALSE)
    ```

1. Download the RDS file from your nf-core/differentialabundance results (see [Shiny app](https://nf-co.re/differentialabundance/1.5.0/docs/output/#shiny-app) from the nf-core documentation for file details): 

    ```r 
    download.file("https://bucket.s3-region.amazonaws.com/differentialabundance/results/shinyngs_app/study-name/data.rds", 'data.rds')
    ```

1. Import libraries, read your RDS data, and launch the RShiny app:

    ```r 
    library(shinyngs)
    library(markdown)
    esel <- readRDS("data.rds")
    app <- prepareApp("rnaseq", esel)
    shiny::shinyApp(app$ui, app$server)
    ```

![Explore the RShiny app](./quickstart-demo/assets/rnaseq-diffab-rshiny-app-explore.gif)

### Xpra: Visualize genetic variants with IGV

Studios and Xpra remote desktop technology enables many interactive analysis and troubleshooting workflows. One such workflow is to perform genetic variant visualization using IGV desktop, a powerful open-source tool for the visual exploration of genomic data. This section demonstrates how to add public data from the 1000 Genomes project to your workspace, set up an Xpra environment with IGV desktop pre-installed, and explore a variant of interest. 

#### Create an AWS Batch compute environment 

Studios requires an AWS Batch compute environment. If you do not have an existing compute environment with at least 2 CPUs available, create one with the following attributes:

- **Region**: To reduce costs, your compute environment should be in the same region as your data. To browse the 1000 Genomes public data optimally, select **us-east-1**.
- **Provisioning model**: Use **On-demand** EC2 instances. 
- Studios does not support AWS Fargate. Do not enable **Use Fargate for head job**. 
- At least 2 available CPUs .

#### Add public data using Data Explorer

Add the 1000 Genomes S3 bucket to your workspace using Data Explorer:

1. From the **Data Explorer** tab, select **Add cloud bucket**. 
1. Specify the bucket details:
    - **Provider**: AWS
    - **Bucket path**: `s3://1000genomes`
    - A unique **Name** for the bucket, such as `1000G`.
    - **Credentials**: Select **Public** from the dropdown menu.
    - An optional bucket **Description**.
1. Select **Add**.

#### Create an Xpra studio 

From the **Data Studios** tab, select **Add a data studio** and complete the following:
- Select the latest **Xpra** container image template from the list.
- Select your AWS Batch compute environment. 
:::note
Studios compete for computing resources when sharing compute environments. Ensure your compute environment has sufficient resources to run both your pipelines and data studio sessions. The default CPU and memory allocation for a studio is 2 CPUs and 8192 MB RAM. 
:::
- Mount the 1000 Genomes S3 bucket you added previously using Data Explorer. 
- Optional: Enter CPU and memory allocations.
- Check **Install Conda packages** and paste the following into the YML field:

    ```yaml 
    channels:
    - conda-forge
    - bioconda
    dependencies:
    - igv
    - samtools
    ```

- Select **Add** or choose to **Add and start** the studio immediately.
- If you chose to **Add** the studio in the preceding step, select **Connect** in the options menu to open the studio in a new browser tab. 

#### View variants in IGV desktop

1. In the Xpra terminal, run `igv` to open IGV desktop.
1. In IGV, change the genome version to hg19.
1. Select **File**, then **Load from file**, then navigate to `/workspace/data/xpra-1000Genomes/phase3/data/HG00096/high_coverage_alignment` and select the `.bai` file, as shown below:

    ![Load BAM file in IGV desktop](./_images/xpra-data-studios-IGV-load-bam.png)

1. Search for PCSK9 and zoom into one of the exons of the gene. If you are on genome version hg19 and everything worked as expected, you should see a coverage graph and reads as shown below:

    ![BAM file view](./_images/xpra-data-studios-IGV-view-bam.png)

### VS Code: Create an interactive Nextflow developer environment

Using Studios with VS Code allows you to set up a portable and interactive Nextflow developer environment that contains all the tools you need to code and run Nextflow pipelines. This section will demonstrate how to set up a VS Code Studio with Conda, nf-core tools, and Apptainer, add public data and run the nf-core/fetchngs pipeline with the `test` profile, and create a VS Code project to start coding your own Nextflow pipelines.  

#### Create an AWS Batch compute environment 

Studios requires an AWS Batch compute environment. If you do not have an existing compute environment with at least 4 CPUs available, create one with the following attributes:

- **Region**: To reduce costs, your compute environment should be in the same region as your data. To use the iGenomes public data bucket that contains the nf-core/fetchngs `test` profile data, select **eu-west-1**.
- Use **On-demand** EC2 instances. 
- Studios does not support AWS Fargate. Do not enable **Use Fargate for head job**. 
- At least 4 available CPUs 

#### Add public data using Data Explorer 

The nf-core/fetchngs pipeline uses data from the NGI iGenomes public dataset for its `test` profile. To add this data to your workspace:

1. From the **Data Explorer** tab, select **Add cloud bucket**.
1. Specify the bucket details:
      - **Provider**: AWS
      - **Bucket path**: `s3://ngi-igenomes/test-data/`
      - A unique **Name** for the bucket: "ngi-igenomes-test-data", or similar
      - **Credentials**: **Public**.
      - An optional bucket **Description**.
1. Select **Add**.

#### Create a VS Code studio 

From the **Data Studios** tab, select **Add a data studio** and complete the following:
- Select the latest **VS Code** container image template from the list.
- Select your AWS Batch compute environment. 
:::note
Studios compete for computing resources when sharing compute environments. Shared compute environments must have sufficient resources to run both your pipelines and studio sessions.
:::
- Mount data using Data Explorer: For the purposes of this guide, mount the NGI iGenomes S3 bucket you added previously. 
- Allocate at least 4 CPUs and 8192 MB RAM. 
- Select **Add** or choose to **Add and start** the studio immediately.
- If you chose to **Add** the studio in the preceding step, select **Connect** in the options menu to open the studio in a new browser tab. 

#### Customize your development environment

