---
title: Local execution
description: "Use Fusion with the Nextflow local executor and cloud storage"
date: "23 Aug 2024"
tags: [fusion, storage, compute, local, s3]
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

With Fusion, you can run Nextflow pipelines using the local executor and a cloud storage bucket as the pipeline scratch directory. This
is useful to scale your pipeline execution vertically with a large compute instance, without the need to allocate
a large storage volume for temporary pipeline data.

## Nextflow CLI

:::tip
This configuration requires Docker or a similar container engine to run pipeline tasks.
:::

<Tabs>
<TabItem value="AWS S3" label="AWS S3" default>

  1. Set `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables to grant Nextflow and Fusion access to your storage credentials. See [Credentials](https://www.nextflow.io/docs/latest/google.html#credentials) for more information.

  1. Add the following to your `nextflow.config` file:

      ```groovy
      wave.enabled = true
      docker.enabled = true
      fusion.enabled = true
      fusion.exportStorageCredentials = true
      ```

  1. Run the pipeline with the Nextflow run command:

      ```
      nextflow run <PIPELINE_SCRIPT> -w s3://<S3_BUCKET>/work
      ```

      Replace the following:
      - `PIPELINE_SCRIPT`: your pipeline Git repository URI
      - `S3_BUCKET`: your S3 bucket name

  :::tip
  To achieve optimal performance, set up an SSD volume as the temporary directory.
  :::

  :::warning
  The option `fusion.exportStorageCredentials` leaks credentials on the task launcher script created by Nextflow.
  This option should only be used for testing and development purposes.
  :::

</TabItem>
<TabItem value="Azure Blob Storage" label="Azure Blob Storage" default>

  1. Set `AZURE_STORAGE_ACCOUNT_NAME` and `AZURE_STORAGE_ACCOUNT_KEY` or `AZURE_STORAGE_SAS_TOKEN` environment variables to grant Nextflow and Fusion access to your storage credentials. See [Credentials](https://www.nextflow.io/docs/latest/google.html#credentials) for more information.

  1. Add the following to your `nextflow.config` file:

      ```groovy
      wave.enabled = true
      docker.enabled = true
      fusion.enabled = true
      fusion.exportStorageCredentials = true
      ```

  1. Run the pipeline with the Nextflow run command:

      ```
      nextflow run <PIPELINE_SCRIPT> -w az://<BLOB_STORAGE>/scratch
      ```

      Replace the following:
      - `<PIPELINE_SCRIPT>`: your pipeline Git repository URI
      - `<BLOB_STORAGE>`: your Azure Blob Storage

  :::tip
  To achieve optimal performance, set up an SSD volume as the temporary directory.
  :::

  :::warning
  The option `fusion.exportStorageCredentials` leaks credentials on the task launcher script created by Nextflow.
  This option should only be used for testing and development purposes.
  :::

</TabItem>
<TabItem value="Google Cloud Storage" label="Google Cloud Storage" default>

  1. Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable with your service account JSON key to grant Nextflow and Fusion access to your storage credentials. See [Credentials](https://www.nextflow.io/docs/latest/google.html#credentials) for more information.

  1. Add the following to your `nextflow.config` file:

      ```groovy
      wave.enabled = true
      docker.enabled = true
      fusion.enabled = true
      fusion.exportStorageCredentials = true
      ```

  1. Run the pipeline with the Nextflow run command:

      ```
      nextflow run <PIPELINE_SCRIPT> -w gs://<GCS_BUCKET>/work
      ```

      Replace the following:
      - `<PIPELINE_SCRIPT>`: your pipeline Git repository URI
      - `<GCS_BUCKET>`: your Google Cloud Storage bucket to which you have read-write access


  :::tip
  To achieve optimal performance, set up an SSD volume as the temporary directory.
  :::

  :::warning
  The option `fusion.exportStorageCredentials` leaks credentials on the task launcher script created by Nextflow.
  This option should only be used for testing and development purposes.
  :::

</TabItem>
<TabItem value="MinIO" label="MinIO" default>

  1. Run a local instance of MinIO using Docker:

      ```
      docker run -p 9000:9000 \
        --rm -d -p 9001:9001 \
        -e "MINIO_ROOT_USER=admin" \
        -e "MINIO_ROOT_PASSWORD=secret" \
        quay.io/minio/minio server /data --console-address ":9001"
      ```

  1. Open the MinIO console `http://localhost:9001`

  1. Create storage credentials and a bucket.

  1. Set `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables to grant Nextflow and Fusion access to your storage credentials. See [Credentials](https://www.nextflow.io/docs/latest/google.html#credentials) for more information.

  1. Add the following to your `nextflow.config` file:

      ```groovy
      aws.client.endpoint = 'http://localhost:9000'
      aws.client.s3PathStyleAccess = true
      wave.enabled = true
      docker.enabled = true
      fusion.enabled = true
      fusion.exportStorageCredentials = true
      ```

  1. Run the pipeline with the Nextflow run command:

      ```
      nextflow run <PIPELINE_SCRIPT> -w s3://<S3_BUCKET>/work
      ```

      Replace the following:
      - `PIPELINE_SCRIPT`: your pipeline Git repository URI
      - `S3_BUCKET`: your S3 bucket name


  :::tip
  To achieve optimal performance, set up an SSD volume as the temporary directory.
  :::

  :::warning
  The option `fusion.exportStorageCredentials` leaks credentials on the task launcher script created by Nextflow.
  This option should only be used for testing and development purposes.
  :::

</TabItem>
<TabItem value="Oracle Object Storage" label="Oracle Object Storage" default>

  :::note
  [Oracle Object Storage](https://www.oracle.com/cloud/storage/object-storage/) relies on the S3-like API compatibility provided by Oracle storage and not by native Nextflow and Fusion support.
  It may not support all Nextflow and Fusion features.
  :::

  1. Set `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables to grant Nextflow and Fusion access to your storage credentials.

  1. Add the following to your `nextflow.config` file:

      ```groovy
      aws.region = '<AWS_REGION>'
      aws.client.endpoint = 'https://<BUCKET_NAMESPACE>.compat.objectstorage.<AWS_REGION>.oraclecloud.com'
      aws.client.protocol = 'https'
      aws.client.signerOverride = 'AWSS3V4SignerType'
      aws.client.s3PathStyleAccess = true
      wave.enabled = true
      docker.enabled = true
      docker.containerOptions = '-e FUSION_AWS_REGION=<AWS_REGION>'
      fusion.enabled = true
      fusion.exportStorageCredentials = true
      ```

      Replace the following:
      - `<AWS_REGION>`: your AWS region
      - `<BUCKET_NAMESPACE>`: your bucket name

  1. Run the pipeline with the Nextflow run command:

      ```
      nextflow run <PIPELINE_SCRIPT> -w s3://<BUCKET>/work
      ```

      Replace the following:
      - `PIPELINE_SCRIPT`: your pipeline Git repository URI
      - `BUCKET`: your bucket

  :::tip
  To achieve optimal performance, set up an SSD volume as the temporary directory.
  :::

  :::warning
  The option `fusion.exportStorageCredentials` leaks credentials on the task launcher script created by Nextflow.
  This option should only be used for testing and development purposes.
  :::

</TabItem>
</Tabs>